# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
The main dataset for this project is the BankMarketing dataset ("https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_train.csv). This dataset contains both categorical and numerical features (20) and target/label for supervised classification ('y'). The aim of the project is to predict if a client will subscribe a term deposit (binary class). For that different approaches were carried out based on the following workflow: 

![GitHub pipelin](/images/creating-and-optimizing-an-ml-pipeline.png)

Concretely, a **LR (logistic regression) baseline** was provided through train.py script. This was totally parametrizable and will enable the Hyperdrive to explore a hyperparameter space using the sampling method that we choose (in this case it was **RandomParameterSampling** choosen). After tuning the hyperparameters using **Hyperdrive** and getting an optimal model, the AutoML process was carried out for both comparison. The best model from both processes are saved in order to compare their performance. 




From both approaches in the pipeline (HD and AutoML) the best performing model was a AutoML. Concretely VotingEmsemble with an accuracy of 0.91763. See below both accuracies of best runs on both approaches.

Accuracy of best run HyperDrive : 0.9074355083459787
Accuracy of best model AutoML (VotingEnsemble) : 0.91763




## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**


The parameter sampler chosen is **RandomSamplingParameter**. It supports discrete and continuous hyperparameters. It is perfect for first approaches and for non exhaustive search space exploration (based on sampling distribution selected). This makes this approach less expensive than other choices as GridSampling.

Early stopping chosen was **MedianStoppingPolicy** based on Google Vizier: A Service for Black-Box Optimization. The main advantage of this policy is that it allows us to be conservative (incurring in less costs) and early stop without losing promising processes. From Microsoft heuristic: 25%-35% savings with no loss on primary metric.


## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

## Proof of cluster clean up
Deleted on notebook: cpu_cluster.delete()
